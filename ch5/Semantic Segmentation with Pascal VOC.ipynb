{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()  \n",
    "prefix = 'pascalvoc-segmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix)\n",
    "s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix)\n",
    "s3_train_annotation_data = 's3://{}/{}/input/train_annotation'.format(bucket, prefix)\n",
    "s3_validation_annotation_data = 's3://{}/{}/input/validation_annotation'.format(bucket, prefix)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "print(s3_train_data)\n",
    "print(s3_validation_data)\n",
    "print(s3_train_annotation_data)\n",
    "print(s3_validation_annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    " \n",
    "role = get_execution_role()\n",
    "\n",
    "seg = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    sagemaker_session = sess,\n",
    "                                    train_instance_count = 1, \n",
    "                                    train_instance_type = 'ml.g4dn.xlarge',\n",
    "                                    output_path = s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.set_hyperparameters(backbone='resnet-50', \n",
    "                             algorithm='fcn',              \n",
    "                             use_pretrained_model=True, \n",
    "                             num_classes=21,\n",
    "                             epochs=30,\n",
    "                             num_training_samples=1464) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm uses to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation_data, distribution='FullyReplicated', \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation_data, distribution='FullyReplicated', \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data and the availability of the type of instances. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out training loss on the training data, which is the pixel-wise cross-entropy loss as described in the algorithm papers. The data logs will also print out pixel-wise label accuracy and mean intersection-over-union (mIoU) on the validation data after a run of the dataset once or one epoch. These metrics measure the quality of the model under training.\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_predictor = seg.deploy(initial_instance_count=1, instance_type='ml.c5.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O test.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/105.horse/105_0016.jpg\n",
    "filename = 'test.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the image to bytearray before we supply it to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "im = PIL.Image.open(filename)\n",
    "im.save(filename, \"JPEG\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(im)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_predictor.content_type = 'image/jpeg'\n",
    "seg_predictor.accept = 'image/png'\n",
    "\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)\n",
    "\n",
    "response = seg_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 21\n",
    "mask = np.array(Image.open(io.BytesIO(response)))\n",
    "plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_predictor.content_type = 'image/jpeg'\n",
    "seg_predictor.accept = 'application/x-protobuf'\n",
    "response = seg_predictor.predict(img)\n",
    "\n",
    "results_file = 'results.rec'\n",
    "with open(results_file, 'wb') as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.record_pb2 import Record\n",
    "import mxnet as mx\n",
    "\n",
    "rec = Record()\n",
    "recordio = mx.recordio.MXRecordIO(results_file, 'r')\n",
    "protobuf = rec.ParseFromString(recordio.read())\n",
    "\n",
    "values = list(rec.features[\"target\"].float32_tensor.values)\n",
    "shape = list(rec.features[\"shape\"].int32_tensor.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape)\n",
    "print(len(values))\n",
    "mask = np.reshape(np.array(values), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_probs = mask[0,:,0,0]\n",
    "print(pixel_probs)\n",
    "print(np.argmax(pixel_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
